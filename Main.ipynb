{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练char2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出每个cell的运行时间\n",
    "%load_ext autotime\n",
    "# https://github.com/cpcloud/ipython-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 998 µs\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.58 s\n"
     ]
    }
   ],
   "source": [
    "root_path = \"data/round1/train/\"\n",
    "w2v_input_path = \"model_file/char2vec_prepareData.txt\"\n",
    "w2v_output_path = \"model_file/char2vec.model\"\n",
    "\n",
    "from Model import Char2VecTrainer\n",
    "char2vec = Char2VecTrainer(root=root_path,w2v_file_path=w2v_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.52 s\n"
     ]
    }
   ],
   "source": [
    "char2vec.prepare_data()\n",
    "char2vec.train(w2v_output_path,emb_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v的模型维度是：256\n",
      "w2v的模型的词表总长是：2301\n",
      "time: 48.9 ms\n"
     ]
    }
   ],
   "source": [
    "char2vec_model = char2vec.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.94 s\n"
     ]
    }
   ],
   "source": [
    "from common.Entity import Document\n",
    "from common.Utils import scan_files\n",
    "from Data import DataSet\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "file_names = scan_files(root_path)\n",
    "\n",
    "rs = ShuffleSplit(n_splits=1, test_size=.15, random_state=2019)\n",
    "train_idx,test_idx = next(rs.split(file_names))\n",
    "\n",
    "train_file_names = [file_names[idx] for idx in train_idx]\n",
    "test_file_names = [file_names[idx] for idx in test_idx]\n",
    "\n",
    "whole_set = DataSet(root_path,file_names,vocab_size=-1)\n",
    "char2idx = whole_set.char2idx\n",
    "del whole_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3246"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.96 ms\n"
     ]
    }
   ],
   "source": [
    "len(char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.56 s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vec_size = char2vec_model.wv.vector_size\n",
    "emb_matrix = np.zeros(vec_size)\n",
    "\n",
    "def random_vec(vec_size):\n",
    "    vec = np.random.random(size=vec_size)\n",
    "    vec = vec - vec.mean()\n",
    "    return vec\n",
    "\n",
    "for c in char2idx.keys():\n",
    "    if c is \"_padding\":\n",
    "        char2idx[c] = 0\n",
    "    elif c is \"_unk\":\n",
    "        emb = random_vec(vec_size)\n",
    "        emb_matrix = np.vstack((emb_matrix,emb))\n",
    "        char2idx[c] = 1\n",
    "    else:\n",
    "        if c in [\" \",\"\\n\"]:\n",
    "            idx = emb_matrix.shape[0]\n",
    "            emb = random_vec(vec_size)\n",
    "            emb_matrix = np.vstack((emb_matrix,emb))\n",
    "            char2idx[c] = idx\n",
    "        elif c not in char2vec_model.wv.vocab.keys():\n",
    "            idx = char2idx[\"_unk\"]\n",
    "            char2idx[c] = idx\n",
    "        else:\n",
    "            idx = emb_matrix.shape[0]\n",
    "            emb = char2vec_model.wv[c]\n",
    "            emb_matrix = np.vstack((emb_matrix,emb))\n",
    "            char2idx[c] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2301"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "len(char2vec_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取并切分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 791 ms\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "rs = ShuffleSplit(n_splits=1, test_size=.20, random_state=2019)\n",
    "train_idx,val_idx = next(rs.split(train_file_names))\n",
    "\n",
    "train_file_names = [file_names[idx] for idx in train_idx]\n",
    "val_file_names = [file_names[idx] for idx in val_idx]\n",
    "\n",
    "trainset = DataSet(root_path,train_file_names,char2idx)\n",
    "valset = DataSet(root_path,val_file_names,char2idx)\n",
    "testset = DataSet(root_path,test_file_names,char2idx)\n",
    "\n",
    "# 持久化\n",
    "pickle.dump(testset,open('pickle_file/testset.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量化 + 滑动窗切分句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.95 s\n"
     ]
    }
   ],
   "source": [
    "from Data import DataProcessor\n",
    "import pickle\n",
    "data_processors = []\n",
    "\n",
    "for dataset in [trainset,valset,testset]:\n",
    "    processor = DataProcessor(dataset).data4NER(window=70,pad=10)\n",
    "    data_processors.append(processor)\n",
    "\n",
    "# 持久化\n",
    "pickle.dump(data_processors,open('pickle_file/data_processors.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建X-Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30780, 90) (30780, 90, 1)\n",
      "(7647, 90) (7647, 90, 1)\n",
      "(7161, 90)\n",
      "time: 5.06 s\n"
     ]
    }
   ],
   "source": [
    "from Data import DataProcessor\n",
    "from typing import List\n",
    "import pickle\n",
    "\n",
    "data_processors = pickle.load(open('pickle_file/data_processors.pkl','rb')) #type:List[DataProcessor]\n",
    "\n",
    "train_X,train_Y = data_processors[0].get_ner_data()\n",
    "train_Y = np.expand_dims(train_Y,-1)\n",
    "\n",
    "val_X,val_Y = data_processors[1].get_ner_data()\n",
    "val_Y = np.expand_dims(val_Y,-1)\n",
    "\n",
    "test_X,_ = data_processors[2].get_ner_data()\n",
    "\n",
    "print(train_X.shape,train_Y.shape)\n",
    "print(val_X.shape,val_Y.shape)\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.,    0.,    0., ...,  754.,    2.,    2.],\n",
       "       [ 529.,  437.,  511., ...,  529.,  437.,  511.],\n",
       "       [ 104.,   94.,  309., ...,  437.,  511.,   68.],\n",
       "       ...,\n",
       "       [ 996.,   24.,  220., ..., 1045.,   24.,  861.],\n",
       "       [  98.,  169.,   21., ...,   24.,  454., 1335.],\n",
       "       [   3., 1841.,   24., ...,    0.,    0.,    0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.99 ms\n"
     ]
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 90)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 90, 256)           590080    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 90, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 90, 1024)          3149824   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 90, 1024)          0         \n",
      "_________________________________________________________________\n",
      "crf_2 (CRF)                  (None, 90, 16)            16688     \n",
      "=================================================================\n",
      "Total params: 3,756,592\n",
      "Trainable params: 3,166,512\n",
      "Non-trainable params: 590,080\n",
      "_________________________________________________________________\n",
      "开始训练啦！！\n",
      "============================================================\n",
      "Train on 30780 samples, validate on 7647 samples\n",
      "Epoch 1/1\n",
      "30780/30780 [==============================] - 147s 5ms/step - loss: 0.3321 - crf_viterbi_accuracy: 0.8845 - val_loss: 0.2553 - val_crf_viterbi_accuracy: 0.8907\n",
      "time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "from Model import BiLstmCrfTrainer\n",
    "from Data import CATEGORY\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 1\n",
    "\n",
    "model = BiLstmCrfTrainer(category_count = len(CATEGORY)+1,\n",
    "                         seq_len = train_X.shape[1],\n",
    "                         lstm_units=512,\n",
    "                         vocab_size = emb_matrix.shape[0],\n",
    "                         emb_matrix = emb_matrix).build()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_crf_viterbi_accuracy', patience=2, mode='max')\n",
    "\n",
    "print('开始训练啦！！')\n",
    "print(20*\"===\")\n",
    "history = model.fit(train_X,train_Y,batch_size=BATCH_SIZE,\n",
    "                    epochs = EPOCH,\n",
    "                    class_weight=\"auto\",\n",
    "                    callbacks = [early_stopping],\n",
    "                    validation_data = (val_X,val_Y,)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 825 ms\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "time = datetime.datetime.now()\n",
    "model.save(filepath=\"model_file/bi_lstm_crf_{}_{}_{}_{}.h5\".format(str(time.month),str(time.day),str(time.hour),str(time.minute)),overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_contrib\n",
    "import pickle\n",
    "\n",
    "model = keras.models.load_model(\"model_file/bi_lstm_crf_12_1_20_36.h5\",\n",
    "                                custom_objects={\"CRF\": keras_contrib.layers.CRF, \"crf_loss\": keras_contrib.losses.crf_loss,\n",
    "                                                \"crf_viterbi_accuracy\": keras_contrib.metrics.crf_viterbi_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7161/7161 [==============================] - 60s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "data_processors = pickle.load(open('pickle_file/data_processors.pkl','rb')) #type:List[DataProcessor]\n",
    "test_X,_ = data_processors[2].get_ner_data()\n",
    "\n",
    "preds = model.predict(test_X, batch_size=16, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【严格相交】F1:0.6186  -  Predicition:0.5890  -  Recall:0.6514\n",
      "【不严格相交】F1:0.7127  -  Predicition:0.6786  -  Recall:0.7504\n"
     ]
    }
   ],
   "source": [
    "from Evaluator import *\n",
    "from common.Entity import Document\n",
    "from Data import DataSet\n",
    "from typing import List\n",
    "\n",
    "testset = pickle.load(open('pickle_file/testset.pkl','rb')) # type:DataSet\n",
    "pre_docs = merge_preds4ner(testset,data_processors[2],preds) # type:List[Document]\n",
    "source_docs = testset.docs\n",
    "\n",
    "f1,prediction,recall = f1_score4ner(pre_docs,source_docs,'all')\n",
    "print(\"【严格相交】F1:{:.4f}  -  Predicition:{:.4f}  -  Recall:{:.4f}\".format(f1,prediction,recall))\n",
    "\n",
    "f1,prediction,recall = f1_score4ner(pre_docs,source_docs,'others')\n",
    "print(\"【不严格相交】F1:{:.4f}  -  Predicition:{:.4f}  -  Recall:{:.4f}\".format(f1,prediction,recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 换biLSTM-LAN模型来实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 90)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 90, 256)      589312      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 90, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 90, 512)      1050624     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 90, 512)      794624      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 90, 1024)     0           bidirectional_1[0][0]            \n",
      "                                                                 attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 90, 512)      2623488     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 90, 16)       532480      bidirectional_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 5,590,528\n",
      "Trainable params: 5,001,216\n",
      "Non-trainable params: 589,312\n",
      "__________________________________________________________________________________________________\n",
      "开始训练啦！！\n",
      "============================================================\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\tf14_py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 30780 samples, validate on 7647 samples\n",
      "Epoch 1/50\n",
      "30780/30780 [==============================] - 629s 20ms/step - loss: 0.3467 - acc: 0.8903 - val_loss: 0.3191 - val_acc: 0.8965\n",
      "Epoch 2/50\n",
      "30780/30780 [==============================] - 623s 20ms/step - loss: 0.2408 - acc: 0.9200 - val_loss: 0.2969 - val_acc: 0.9053\n",
      "Epoch 3/50\n",
      "30780/30780 [==============================] - 623s 20ms/step - loss: 0.2058 - acc: 0.9304 - val_loss: 0.2909 - val_acc: 0.9079\n",
      "Epoch 4/50\n",
      "30780/30780 [==============================] - 623s 20ms/step - loss: 0.1834 - acc: 0.9373 - val_loss: 0.2861 - val_acc: 0.9091\n",
      "Epoch 5/50\n",
      "30780/30780 [==============================] - 624s 20ms/step - loss: 0.1668 - acc: 0.9423 - val_loss: 0.2957 - val_acc: 0.9047\n",
      "Epoch 6/50\n",
      "30780/30780 [==============================] - 624s 20ms/step - loss: 0.1537 - acc: 0.9464 - val_loss: 0.2957 - val_acc: 0.9115\n",
      "Epoch 7/50\n",
      "30780/30780 [==============================] - 625s 20ms/step - loss: 0.1432 - acc: 0.9499 - val_loss: 0.3066 - val_acc: 0.9085\n",
      "Epoch 8/50\n",
      "30780/30780 [==============================] - 624s 20ms/step - loss: 0.1339 - acc: 0.9528 - val_loss: 0.2929 - val_acc: 0.9140\n",
      "Epoch 9/50\n",
      "30780/30780 [==============================] - 624s 20ms/step - loss: 0.1263 - acc: 0.9553 - val_loss: 0.3013 - val_acc: 0.9099\n",
      "Epoch 10/50\n",
      "30780/30780 [==============================] - 625s 20ms/step - loss: 0.1184 - acc: 0.9579 - val_loss: 0.3098 - val_acc: 0.9112\n",
      "time: 1h 44min 9s\n"
     ]
    }
   ],
   "source": [
    "from Model import BiLstm_Lan_Trainer\n",
    "from Prepare_sents import CATEGORY\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCH = 50\n",
    "\n",
    "model = BiLstm_Lan_Trainer(category_count = len(CATEGORY)+1,\n",
    "                         seq_len = train_X.shape[1],\n",
    "                         lstm_units=[256,256],\n",
    "                         vocab_size = emb_matrix.shape[0],\n",
    "                         emb_matrix = emb_matrix).build()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=2, mode='max')\n",
    "\n",
    "print('开始训练啦！！')\n",
    "print(20*\"===\")\n",
    "history = model.fit(train_X,train_Y,batch_size=BATCH_SIZE,\n",
    "                    epochs = EPOCH,\n",
    "                    class_weight=\"auto\",\n",
    "                    callbacks = [early_stopping],\n",
    "                    validation_data = (val_X,val_Y)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7161/7161 [==============================] - 59s 8ms/step\n",
      "【严格相交】F1:0.7369  -  Predicition:0.6846  -  Recall:0.7979\n",
      "【不严格相交】F1:0.8043  -  Predicition:0.7472  -  Recall:0.8710\n",
      "time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(test_X, batch_size=16, verbose=True)\n",
    "from Evaluator import merge_preds,f1_score\n",
    "from Prepare_sents import Sentences\n",
    "testset = pickle.load(open('pickle_data/testset.pkl','rb'))\n",
    "pre_docs = merge_preds(testset,preds,70,10)\n",
    "source_docs = testset.docs\n",
    "f1,prediction,recall = f1_score(pre_docs,source_docs,'all')\n",
    "print(\"【严格相交】F1:{:.4f}  -  Predicition:{:.4f}  -  Recall:{:.4f}\".format(f1,prediction,recall))\n",
    "\n",
    "f1,prediction,recall = f1_score(pre_docs,source_docs,'others')\n",
    "print(\"【不严格相交】F1:{:.4f}  -  Predicition:{:.4f}  -  Recall:{:.4f}\".format(f1,prediction,recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
